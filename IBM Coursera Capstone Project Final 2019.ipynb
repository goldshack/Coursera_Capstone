{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import statistics\n",
    "pd.set_option('display.max_columns', None)\n",
    "from statistics import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print('Success!')\n",
    "elif response.status_code == 404:\n",
    "    print('Not Found.')\n",
    "    \n",
    "print(url)\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "table_body=soup.find('tbody')\n",
    "trs = table_body.find_all('tr')\n",
    "rows = []\n",
    "for tr in trs:\n",
    "    i = tr.find_all('td')\n",
    "    if i:\n",
    "        rows.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table = []\n",
    "for row in rows:\n",
    "    postalcode = row[0].text.strip()\n",
    "    borough = row[1].text.strip()\n",
    "    neighborhood = row[2].text.strip()\n",
    "    if borough != 'Not assigned':             \n",
    "        if neighborhood == 'Not assigned':\n",
    "            neighborhood = borough\n",
    "        list_table.append([postalcode, borough, neighborhood])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_table, columns=['PostalCode', 'Borough', 'Neighborhood'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops rows where Borough = 'Not assigned' if any\n",
    "df.drop(df.loc[df['Borough']=='Not assigned'].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('PostalCode').agg({'Borough':'first','Neighborhood': ', '.join}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['PostalCode'] == 'M5A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = pd.read_csv('http://cocl.us/Geospatial_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.rename(columns={'Postal Code':'PostalCode'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df, loc_df, on=\"PostalCode\", how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Coordinates\n",
    "address = 'Toronto'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"toronto_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Toronto are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of Toronto using latitude and longitude values\n",
    "map_toronto = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, label in zip(df2['Latitude'], df2['Longitude'], df2['Neighborhood']):\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_toronto)  \n",
    "    \n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '23QCEME0X05NQPDAGB1K0WYP3PRWCIJ0ALLDPMBEY1ZXTZPG' # Foursquare ID\n",
    "CLIENT_SECRET = 'A5YTYTWO5L2D11VOQDEDD3U5YJI1PNQ4C4XPSTVQEU3SHWYV' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + CLIENT_ID)\n",
    "print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the Woburn Neighborhood\n",
    "df2.loc[3, 'Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_latitude = df2.loc[3, 'Latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = df2.loc[3, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "neighborhood_name = df2.loc[3, 'Neighborhood'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the top 100 venues that are in Woburn within a radius of 500 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 500\n",
    "LIMIT = 100\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(url).json()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at all Neighborhoods in Toronto\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_venues= getNearbyVenues(names=df2['Neighborhood'],\n",
    "                                   latitudes=df2['Latitude'],\n",
    "                                   longitudes=df2['Longitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Analyzing all the Toronto Neighborhoods\n",
    "# one hot encoding\n",
    "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\n",
    "toronto_onehot = toronto_onehot[fixed_columns]\n",
    "\n",
    "toronto_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post1 = []\n",
    "for col in toronto_onehot.columns:\n",
    "    if ('Restaurant') in col:\n",
    "        post1.append(col)\n",
    "\n",
    "\n",
    "toronto_onehot['Total_Restaurant']=0\n",
    "\n",
    "for i in post1:\n",
    "    toronto_onehot['Total_Restaurant'] += toronto_onehot[i]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_onehot['Total_Restaurant'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_onehot2=toronto_onehot[['Total_Restaurant','Neighborhood']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group rows by neighborhood and by taking the mean of the frequency of occurrence of each category\n",
    "toronto_grouped1 = toronto_onehot2.groupby('Neighborhood').agg({'Total_Restaurant':np.mean}).reset_index()\n",
    "toronto_grouped2 = toronto_onehot2.groupby('Neighborhood').agg({'Total_Restaurant':np.sum}).reset_index()\n",
    "toronto_grouped1 = toronto_grouped1.sort_values(by='Total_Restaurant', ascending=False)\n",
    "toronto_grouped1['Total_Restaurant_Frequency']=toronto_grouped1['Total_Restaurant']\n",
    "toronto_grouped1 = toronto_grouped1.drop(columns=['Total_Restaurant'])\n",
    "toronto_grouped_final= toronto_grouped1.join(toronto_grouped2.set_index('Neighborhood'), on='Neighborhood')\n",
    "toronto_grouped_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged = df2\n",
    "\n",
    "\n",
    "toronto_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final = toronto_merged.join(toronto_grouped_final.set_index('Neighborhood'), on='Neighborhood')\n",
    "toronto_merged_final=toronto_merged_final[['Borough','Neighborhood','PostalCode','Latitude','Longitude','Total_Restaurant','Total_Restaurant_Frequency']]\n",
    "toronto_merged_final=toronto_merged_final.sort_values(by='Total_Restaurant', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final=toronto_merged_final[['Borough','Neighborhood','PostalCode','Latitude','Longitude','Total_Restaurant','Total_Restaurant_Frequency']]\n",
    "toronto_merged_final=toronto_merged_final.sort_values(by='Total_Restaurant', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_group=list(toronto_merged_final['Latitude'])\n",
    "lat_list=list(map(str, lat_group))\n",
    "lon_group=list(toronto_merged_final['Longitude'])\n",
    "lon_list = list(map(str, lon_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining AVG Temperature List\n",
    "lst_temp=[]\n",
    "final_lst_temp=[]\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        temp_data=json_data['list'][i]['main']['temp']\n",
    "        lst_temp.append(temp_data)\n",
    "    temp_avg=statistics.mean(lst_temp)\n",
    "    final_lst_temp.append(temp_avg)\n",
    "\n",
    "final_lst_temp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining AVG Humidity List\n",
    "lst_hum=[]\n",
    "final_lst_hum=[]\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        hum_data=json_data['list'][i]['main']['humidity']\n",
    "        lst_hum.append(hum_data)\n",
    "    hum_avg=statistics.mean(lst_hum)\n",
    "    final_lst_hum.append(hum_avg)\n",
    "\n",
    "final_lst_hum[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_temp = np.asarray(final_lst_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_hum=np.asarray(final_lst_hum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final['Average 5-Day Temperature']= final_np_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final['Average 5-Day Humidity'] = final_np_hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['list'][i]['weather'][0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining most frequent weather condition\n",
    "lst_con=[]\n",
    "final_lst_con=[]\n",
    "from collections import Counter\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        con_data=json_data['list'][i]['weather'][0]['description']\n",
    "        lst_con.append(con_data)\n",
    "        c = Counter(lst_con)\n",
    "        common_list = c.most_common(i)\n",
    "        \n",
    "    if common_list[0][1] > common_list[1][1]:\n",
    "        final_lst_con.append(common_list[0][0])\n",
    "    elif common_list[0][1] == common_list[1][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1]==common_list[3][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0]+' & '+ common_list[4][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[2][0] +' & '+ common_list[3][0]+' & '+ common_list[4][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1] == common_list[5][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1] == common_list[5][1] == common_list[6][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0] +' & '+ common_list[7][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_lst_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_con = np.asarray(final_lst_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final['Most Frequent Weather Condition']= final_np_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of Clear Sky Condition\n",
    "\n",
    "lst_con2=[]\n",
    "final_lst_con2=[]\n",
    "from collections import Counter\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        con_data=json_data['list'][i]['weather'][0]['description']\n",
    "        lst_con2.append(con_data)\n",
    "    lst_con2_count = lst_con2.count('clear sky')/40\n",
    "    final_lst_con2.append(lst_con2_count)\n",
    "    lst_con2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_lst_con2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_con2 = np.asarray(final_lst_con2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final['Clear Sky condition frequency']= final_np_con2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import wget\n",
    "import json\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Jeffrey.Lu@ibm.com/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newyork_data.json') as json_data:\n",
    "    newyork_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataframe columns\n",
    "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
    "\n",
    "# instantiate the dataframe\n",
    "neighborhoods = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_data = newyork_data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in neighborhoods_data:\n",
    "    borough = neighborhood_name = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure we have 5 boroughs in the data set\n",
    "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
    "        len(neighborhoods['Borough'].unique()),\n",
    "        neighborhoods.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_latitude = neighborhoods.loc[0, 'Latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = neighborhoods.loc[0, 'Longitude'] # neighborhood longitude value\n",
    "\n",
    "neighborhood_name = neighborhoods.loc[0, 'Neighborhood'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 500\n",
    "LIMIT = 100\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(url).json()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = results['response']['groups'][0]['items']\n",
    "    \n",
    "nearby_venues = json_normalize(venues) # flatten JSON\n",
    "\n",
    "# filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "# filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "# clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_data = neighborhoods\n",
    "newyork_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at all Neighborhoods in New York\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_venues = getNearbyVenues(names=newyork_data['Neighborhood'],\n",
    "                                   latitudes=newyork_data['Latitude'],\n",
    "                                   longitudes=newyork_data['Longitude'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "newyork_onehot = pd.get_dummies(newyork_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "newyork_onehot['Neighborhood'] = newyork_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [newyork_onehot.columns[-1]] + list(newyork_onehot.columns[:-1])\n",
    "newyork_onehot = newyork_onehot[fixed_columns]\n",
    "\n",
    "newyork_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post2 = []\n",
    "for col in newyork_onehot.columns:\n",
    "    if ('Restaurant') in col:\n",
    "        post2.append(col)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_onehot['Total_Restaurant']=0\n",
    "\n",
    "for i in post2:\n",
    "    newyork_onehot['Total_Restaurant'] += newyork_onehot[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_onehot['Total_Restaurant'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_onehot2=newyork_onehot[['Total_Restaurant','Neighborhood']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group rows by neighborhood and by taking the mean of the frequency of occurrence of each category\n",
    "newyork_grouped1 = newyork_onehot2.groupby('Neighborhood').agg({'Total_Restaurant':np.mean}).reset_index()\n",
    "newyork_grouped2 = newyork_onehot2.groupby('Neighborhood').agg({'Total_Restaurant':np.sum}).reset_index()\n",
    "newyork_grouped1 = newyork_grouped1.sort_values(by='Total_Restaurant', ascending=False)\n",
    "newyork_grouped1['Total_Restaurant_Frequency']=newyork_grouped1['Total_Restaurant']\n",
    "newyork_grouped1 = newyork_grouped1.drop(columns=['Total_Restaurant'])\n",
    "newyork_grouped_final= newyork_grouped1.join(newyork_grouped2.set_index('Neighborhood'), on='Neighborhood')\n",
    "newyork_grouped_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged = newyork_data\n",
    "\n",
    "\n",
    "newyork_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final = newyork_merged.join(newyork_grouped_final.set_index('Neighborhood'), on='Neighborhood')\n",
    "newyork_merged_final= newyork_merged_final.sort_values(by='Total_Restaurant', ascending=False)\n",
    "newyork_merged_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_group=list(newyork_merged_final['Latitude'])\n",
    "lat_list=list(map(str, lat_group))\n",
    "lon_group=list(newyork_merged_final['Longitude'])\n",
    "lon_list = list(map(str, lon_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining AVG Temperature List\n",
    "lst_temp=[]\n",
    "final_lst_temp=[]\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        temp_data=json_data['list'][i]['main']['temp']\n",
    "        lst_temp.append(temp_data)\n",
    "    temp_avg=statistics.mean(lst_temp)\n",
    "    final_lst_temp.append(temp_avg)\n",
    "\n",
    "final_lst_temp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining AVG Humidity List\n",
    "lst_hum=[]\n",
    "final_lst_hum=[]\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        hum_data=json_data['list'][i]['main']['humidity']\n",
    "        lst_hum.append(hum_data)\n",
    "    hum_avg=statistics.mean(lst_hum)\n",
    "    final_lst_hum.append(hum_avg)\n",
    "\n",
    "final_lst_hum[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_temp = np.asarray(final_lst_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_hum=np.asarray(final_lst_hum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final['Average 5-Day Temperature']= final_np_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final['Average 5-Day Humidity'] = final_np_hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data['list'][i]['weather'][0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining most frequent weather condition\n",
    "lst_con=[]\n",
    "final_lst_con=[]\n",
    "from collections import Counter\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        con_data=json_data['list'][i]['weather'][0]['description']\n",
    "        lst_con.append(con_data)\n",
    "        c = Counter(lst_con)\n",
    "        common_list = c.most_common(i)\n",
    "        \n",
    "    if common_list[0][1] > common_list[1][1]:\n",
    "        final_lst_con.append(common_list[0][0])\n",
    "    elif common_list[0][1] == common_list[1][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1]==common_list[3][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0]+' & '+ common_list[4][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[2][0] +' & '+ common_list[3][0]+' & '+ common_list[4][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1] == common_list[5][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0])\n",
    "    elif common_list[0][1] == common_list[1][1] == common_list[2][1] == common_list[3][1] == common_list[4][1] == common_list[5][1] == common_list[5][1] == common_list[6][1]:\n",
    "        final_lst_con.append(common_list[0][0] +' & '+ common_list[1][0] +' & '+ common_list[3][0] +' & '+ common_list[4][0] +' & '+ common_list[5][0] +' & '+ common_list[6][0] +' & '+ common_list[7][0])\n",
    "      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_lst_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_con = np.asarray(final_lst_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final['Most Frequent Weather Condition']= final_np_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of Clear Sky Condition\n",
    "\n",
    "lst_con2=[]\n",
    "final_lst_con2=[]\n",
    "from collections import Counter\n",
    "\n",
    "for lat, lon in zip(lat_list, lon_list):\n",
    "    api_address='http://api.openweathermap.org/data/2.5/forecast?lat='+lat+'&lon='+lon+'&units=imperial&APPID=aee82abb5cd39bd3aef65599bde034fa'\n",
    "    json_data= requests.get(api_address).json()\n",
    "    \n",
    "    for i in range(0,40):\n",
    "        con_data=json_data['list'][i]['weather'][0]['description']\n",
    "        lst_con2.append(con_data)\n",
    "    lst_con2_count = lst_con2.count('clear sky')/40\n",
    "    final_lst_con2.append(lst_con2_count)\n",
    "    lst_con2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_lst_con2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np_con2 = np.asarray(final_lst_con2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final['Clear Sky condition frequency']= final_np_con2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining Both Toronto and New York Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=toronto_merged_final.append(newyork_merged_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('PostalCode', axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyzing Neighborhoods in Toronto and New York with Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_grouped = newyork_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "newyork_grouped.drop('Total_Restaurant', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in newyork_grouped['Neighborhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = newyork_grouped[newyork_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = newyork_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(newyork_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(newyork_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 10\n",
    "\n",
    "newyork_grouped_clustering = newyork_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(newyork_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "newyork_merged = newyork_data\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "newyork_merged = newyork_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "newyork_merged.head() # check the last columns!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(newyork_merged['Latitude'], newyork_merged['Longitude'], newyork_merged['Neighborhood'], newyork_merged['Cluster Labels'].fillna(0)):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[int(cluster-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(cluster-1)],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 0, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged['Borough-Neighborhood']= list(zip(newyork_merged['Borough'], newyork_merged['Neighborhood']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 1 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 0, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 2 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 1, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 3 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 2, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 4 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 3, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 5 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 4, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 6 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 5, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 7 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 6, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 8 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 7, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 9 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 8, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 10 New York\n",
    "newyork_merged.loc[newyork_merged['Cluster Labels'] == 9, newyork_merged.columns[[1] + list(range(5, newyork_merged.shape[1]))]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toronto Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "toronto_grouped.drop('Total_Restaurant', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted1 = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted1['Neighborhood'] = toronto_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(toronto_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted1.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted1.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted1.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "toronto_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged.drop('PostalCode',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels'].fillna(0)):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[int(cluster-1)],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[int(cluster-1)],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Cluster 1\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Cluster 2\n",
    "\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Cluster 3\n",
    "\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Cluster 4\n",
    "\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toronto Cluster 5\n",
    "\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing Toronto neighborhoods\n",
    "df_tor=toronto_merged_final.where(toronto_merged_final['Total_Restaurant'] > 10).sort_values(by=['Total_Restaurant_Frequency'], ascending=False)\n",
    "df_tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tor5 = df_tor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing New York neighborhoods\n",
    "\n",
    "df_ny = newyork_merged_final.where(newyork_merged_final['Total_Restaurant'] > 10).sort_values(by=['Total_Restaurant_Frequency'], ascending=False)\n",
    "df_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny5= df_ny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(df_ny5['Neighborhood'],df_ny5['Total_Restaurant_Frequency'], align='center', alpha=0.7)\n",
    "plt.ylabel('Total Restaurant Frequency')\n",
    "plt.title('Top 5 New York Neighborhoods')\n",
    "fig = plt.figure(1, [20, 8])\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(df_tor5['Neighborhood'],df_tor5['Total_Restaurant_Frequency'], align='center', alpha=0.7)\n",
    "plt.ylabel('Total Restaurant Frequency')\n",
    "plt.title('Top 5 Toronto Neighborhoods')\n",
    "fig = plt.figure(1, [20, 8])\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
